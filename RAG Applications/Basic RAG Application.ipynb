{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **1. Installing and Importing Required Libraries**\n",
    "\n",
    "- **Purpose:** Ensure all necessary packages are installed and imported for the RAG pipeline.\n",
    "- **Key Libraries:**\n",
    "    - `langchain`, `langchain-community`, `langchainhub`, `langchain-chroma`, `langchain-openai`, `langchain-text-splitters`: Core LangChain and extensions for LLMs, embeddings, and vector storage.\n",
    "    - `faiss-cpu`, `bs4`: For vector search and HTML parsing.\n",
    "    - `gradio`, `gradio_client`: For building a web UI.\n",
    "    - `ipywidgets`: For interactive widgets in Jupyter."
   ],
   "id": "16f02ad0910c5ca3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "%pip install -U pip setuptools wheel\n",
    "%pip uninstall -y gradio gradio_client numpy\n",
    "%pip install \"numpy>=2.1.0,<3\"\n",
    "%pip install -U langchain langchain-community langchainhub langchain-chroma bs4 langchain-openai langchain-text-splitters faiss-cpu\n",
    "%pip install -U \"gradio>=4.0\"  \"gradio_client>=0.14\"\n",
    "%pip install -U langchain-text-splitters\n",
    "%pip install ipywidgets"
   ],
   "id": "2e004f38e44d2b64",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **2. Environment Setup and API Keys**\n",
    "\n",
    "- **Purpose:** Set up environment variables for API access and user agent identification.\n",
    "- **Key Points:**\n",
    "    - Sets the OpenAI API key and a custom user agent for requests.\n",
    "    - Uses `getpass` as a fallback for secure key entry."
   ],
   "id": "fa3dcaf6ec92dd86"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T04:52:36.780127600Z",
     "start_time": "2025-10-20T04:51:46.262982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, getpass\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnableSequence, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import gradio as gr\n",
    "import os, getpass\n",
    "OPENAI_API_KEY = \"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\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "os.environ[\"USER_AGENT\"] = \"TestLangchainApp1.0/ (Lnrdballen@gmail.com)\"\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "#if not os.environ.get(\"TAVILY_APU_KEY\"):\n",
    "    #os.environ[\"TAVILY_API_KEY\"] = getpass.getpass()"
   ],
   "id": "170135ffb23623d0",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **3. Initialize the LLM and Load Documents from the Web**\n",
    "\n",
    "- **Purpose:** Create a language model instance for answering questions. Fetch documents to build the knowledge base for retrieval.\n",
    "- **Key Points:**\n",
    "    - Uses OpenAI's `gpt-4o-mini` model with deterministic output (`temperature=0`).\n",
    "    - Uses `WebBaseLoader` to load content from specified URLs.\n",
    "    - Custom user agent is passed for identification."
   ],
   "id": "6b71b5d8edc7a911"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T04:54:06.412060Z",
     "start_time": "2025-10-20T04:54:06.407494Z"
    }
   },
   "cell_type": "code",
   "source": "llm = ChatOpenAI(model=\"gpt-4o-mini\",temperature = 0)",
   "id": "2f2109461adb6f23",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T04:54:06.929809Z",
     "start_time": "2025-10-20T04:54:06.421614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#from langchain_community.document_loaders import WebBaseLoader\n",
    "urls = [\n",
    "    \"https://langchain-ai.github.io/langgraph/tutorials/workflows/\",\n",
    "    #place other links here\n",
    "]\n",
    "\n",
    "loader = WebBaseLoader(web_paths= urls, header_template = {\"User-Agent\": os.environ[\"USER_AGENT\"]})\n",
    "docs = loader.load()"
   ],
   "id": "b057b36d14af31d0",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **4. Create Embeddings and Vector Store**\n",
    "\n",
    "- **Purpose:** Convert documents into embeddings and store them for similarity search.\n",
    "- **Key Points:**\n",
    "    - Uses OpenAI's embedding model (`text-embedding-3-small`).\n",
    "    - Splits documents into manageable chunks for better retrieval.\n",
    "    - Stores embeddings in a FAISS vector store."
   ],
   "id": "cbc297f8f70bd5da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T04:54:17.823393Z",
     "start_time": "2025-10-20T04:54:15.616617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model = \"text-embedding-3-small\")\n",
    "#from langchain_community.vectorstores import FAISS\n",
    "#from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000 , chunk_overlap = 200)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "vectorstore = FAISS.from_documents(split_documents, embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ],
   "id": "76aa04df8ec99fab",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **5. Prompt and RAG Chain Construction**\n",
    "\n",
    "- **Purpose:** Define how the LLM should answer questions using retrieved context.\n",
    "- **Key Points:**\n",
    "    - System prompt instructs the LLM to use only provided context.\n",
    "    - `ChatPromptTemplate` structures the prompt for the LLM.\n",
    "    - `RunnableSequence` chains together retrieval, prompt formatting, LLM call, and output parsing."
   ],
   "id": "3302ace62a1fc89b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T04:54:29.733770Z",
     "start_time": "2025-10-20T04:54:29.729873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_parser = StrOutputParser()\n",
    "system_prompt = \"Answer using only the provided context. If unsure, say you don't know.\\n\\nContext:\\n{context}\\nChat history:\\n{chat_history}\"\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{question}\")])\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(i.page_content for i in docs)\n",
    "def format_chat_history(history):\n",
    "    return \"\\n\".join([f\"{h['role']}: {h['content']}\" for h in history]) if history else \"\"\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": RunnableLambda(lambda d: d[\"question\"]) | retriever | format_docs,\n",
    "        \"chat_history\": RunnableLambda(lambda d: format_chat_history(d[\"chat_history\"])),\n",
    "        \"question\": RunnableLambda(lambda d: d[\"question\"])\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ],
   "id": "17c2eb8b866bc6c8",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **6. Gradio Chat Interface**\n",
    "\n",
    "- **Purpose:** Provide a web-based chat interface for user interaction.\n",
    "- **Key Points:**\n",
    "    - Classifies user messages as 'general' or 'document' questions.\n",
    "    - Routes document questions to the RAG chain, others get a default response.\n",
    "    - Maintains chat history and allows temperature adjustment."
   ],
   "id": "dc3ed87adcce6b26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T04:52:36.798644800Z",
     "start_time": "2025-10-20T04:51:48.359998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\",temperature = 0)\n",
    "\n",
    "import gradio as gr\n",
    "def llm_classify(message, llm):\n",
    "    prompt = (\n",
    "        \"Classify the following message as 'general'\"\n",
    "        \"or 'document' (question about provided documents):\\n\"\n",
    "        f\"Message: {message}\\n\"\n",
    "        \"Intent:\"\n",
    "    )\n",
    "    result = llm.invoke(prompt)\n",
    "    intent = result.content.strip().lower()\n",
    "    return intent == \"general\"\n",
    "\n",
    "def is_general_question(message):\n",
    "    prompt = f\"Classify the following message as 'general'hehe or 'document': {message}\"\n",
    "    intent = llm_classify(prompt, llm)\n",
    "    return intent == \"general\"\n",
    "\n",
    "def gradio_chat(message, history, temperature):\n",
    "    history = history or []\n",
    "    history.append({\"role\": \"user\", \"content\": message})\n",
    "    greetings = [\"hi\", \"hello\", \"hey\"]\n",
    "\n",
    "    if message.lower().strip() in greetings:\n",
    "        response = \"Hello! How can I assist you today?\"\n",
    "\n",
    "    elif is_general_question(message):\n",
    "        response = \"I cannot help you with this.\"\n",
    "    else:\n",
    "        payload = {\n",
    "            \"question\": message,\n",
    "            \"chat_history\": history,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "        response = rag_chain.invoke(payload)\n",
    "    history.append({\"role\": \"assistant\", \"content\": response})\n",
    "    return history\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    fn=gradio_chat,\n",
    "    type=\"messages\",  # Use the new format!\n",
    "    additional_inputs=[\n",
    "        gr.Slider(0.0, 1.0, value=0.7, label=\"Temperature\")\n",
    "    ],\n",
    "    title=\"RAG Chatbot\",\n",
    "    description=\"Ask questions about your documents!\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ],
   "id": "bd27479f41901890",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **7. (Optional) Command-Line Chat Loop**\n",
    "\n",
    "- **Purpose:** Allow interaction with the RAG chain via the terminal.\n",
    "- **Key Points:**\n",
    "    - Continuously prompts for user input and appends to chat history.\n",
    "    - Exits on 'exit', 'quit', or 'stop'.\n"
   ],
   "id": "6d689c971c4f6c4c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T04:54:48.935034Z",
     "start_time": "2025-10-20T04:54:43.884967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "while True:\n",
    "    question = input(\"You: \")\n",
    "    if question.lower() in [\"exit\", \"quit\", \"stop\"]:\n",
    "        break\n",
    "    payload = {\n",
    "        \"question\": question,\n",
    "        \"chat_history\": chat_history\n",
    "    }\n",
    "    resp = rag_chain.invoke(payload)\n",
    "    print(resp)\n",
    "    chat_history.append({\"role\": \"human\", \"content\": question})\n",
    "    chat_history.append({\"role\": \"bot\", \"content\": resp})"
   ],
   "id": "9824660add34290f",
   "outputs": [],
   "execution_count": 35
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
